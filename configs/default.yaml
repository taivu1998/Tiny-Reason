project:
  name: "TinyReason-Qwen-1.5B"
  seed: 3407
  output_dir: "outputs"
  adapter_path: "outputs/lora_adapter"

model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"
  max_seq_length: 2048
  load_in_4bit: true

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0
  bias: "none"
  target_modules: 
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

data:
  dataset_name: "openai/gsm8k"
  subset: "main"
  split: "train"
  num_samples: 2000
  test_samples: 50
  num_proc: 2
  validation_split: 0.1
  shuffle: true
  system_prompt: "You are a helpful assistant that solves math problems step-by-step."

training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4  # Effective batch size = 8
  warmup_steps: 50
  max_steps: 250
  learning_rate: 0.0002
  logging_steps: 10
  eval_steps: 50
  save_steps: 50
  save_total_limit: 3
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"